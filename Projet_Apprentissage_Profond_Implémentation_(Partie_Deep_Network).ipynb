{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "CCN-p9rLAM02",
    "3vwYQRYtAQVO"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projet - Apprentissage Profond - Implémentation (Partie Deep Network)\n",
    "\n",
    "**EL KAAKOUR Ahmad & Matthieu RANDRIANTSOA**"
   ],
   "metadata": {
    "id": "ghjljArV_gmT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Ce notebook contient toutes les fonctions d'implémentation de nos modèles. Il est conçu de sorte à s'exécuter sequentiellement."
   ],
   "metadata": {
    "id": "Fs97AZe8_l9a"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RUABdAnG-sbu",
    "ExecuteTime": {
     "end_time": "2024-10-18T18:25:23.310005Z",
     "start_time": "2024-10-18T18:25:20.188972Z"
    }
   },
   "source": [
    "import gzip, numpy, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "id": "v7fTwU6n_s68",
    "ExecuteTime": {
     "end_time": "2024-10-16T09:54:50.450095Z",
     "start_time": "2024-10-16T09:54:50.434582Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Partie 3 : Deep Network"
   ],
   "metadata": {
    "id": "fBm3XiVF_w1D"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des données"
   ],
   "metadata": {
    "id": "CCN-p9rLAM02"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "((data_train,label_train),(data_test,label_test)) = torch.load(gzip.open('data/mnist.pkl.gz'), weights_only=False)"
   ],
   "metadata": {
    "id": "dGbbXmGn_wK5",
    "ExecuteTime": {
     "end_time": "2024-10-18T18:26:36.197332Z",
     "start_time": "2024-10-18T18:26:35.049569Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données"
   ],
   "metadata": {
    "id": "3vwYQRYtAQVO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_dataset = torch.utils.data.TensorDataset(data_train, label_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(data_test, label_test)\n",
    "\n",
    "# Diviser le jeu de données en sous-ensembles\n",
    "generator = torch.Generator().manual_seed(42) # Permet de reproduire le même découpement\n",
    "train_subset, val_subset = torch.utils.data.random_split(training_dataset, [0.8, 0.2], generator=generator) # on divise le jeu de données en 80% pour l'entraînement et 20% pour la validation"
   ],
   "metadata": {
    "id": "wng74gQEAKm_",
    "ExecuteTime": {
     "end_time": "2024-10-18T18:26:36.497951Z",
     "start_time": "2024-10-18T18:26:36.449692Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du modèle de réseau de neurones"
   ],
   "metadata": {
    "id": "97v9qRt0AuxS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variante à trois couches cachées"
   ],
   "metadata": {
    "id": "bzuHPGh2BBBs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DeepNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self,nb_neurons_1:int, nb_neurons_2:int, nb_neurons_3:int):\n",
    "        super(DeepNeuralNetwork,self).__init__()\n",
    "\n",
    "        # première couche cachée\n",
    "        self.hidden_layer_1 = torch.nn.Linear(28*28,nb_neurons_1)\n",
    "        # deuxième couche cachée\n",
    "        self.hidden_layer_2 = torch.nn.Linear(nb_neurons_1, nb_neurons_2)\n",
    "        # troisième couche cachée\n",
    "        self.hidden_layer_3 = torch.nn.Linear(nb_neurons_2, nb_neurons_3)\n",
    "        # couche de sortie\n",
    "        self.output_layer = torch.nn.Linear(nb_neurons_3,  10)\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "QbuNgoK_At-L",
    "ExecuteTime": {
     "end_time": "2024-10-18T18:25:40.077351Z",
     "start_time": "2024-10-18T18:25:40.061711Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Variantes à deux couches cachées"
   ],
   "metadata": {
    "id": "fUwQXLEsBDPs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DeepNeuralNetwork_2(torch.nn.Module):\n",
    "    def __init__(self,nb_neurons_1:int, nb_neurons_2:int):\n",
    "        super(DeepNeuralNetwork,self).__init__()\n",
    "\n",
    "        # première couche cachée\n",
    "        self.hidden_layer_1 = torch.nn.Linear(28*28,nb_neurons_1)\n",
    "        # deuxième couche cachée\n",
    "        self.hidden_layer_2 = torch.nn.Linear(nb_neurons_1, nb_neurons_2)\n",
    "        #couche de sortie\n",
    "        self.output_layer = torch.nn.Linear(nb_neurons_2,  10)\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        #passage en avant\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "HF05oFdQBITN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fonctions d'entrainement"
   ],
   "metadata": {
    "id": "IhurKq5jBnFA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_and_evaluate_2(_model, _training_loader, _validation_loader, _params, device='cpu'):\n",
    "    # Extraire les hyperparamètres depuis params\n",
    "    _nb_epochs = _params.get('nb_epochs', 10)\n",
    "    _loss_func = _params.get('loss_func', torch.nn.CrossEntropyLoss())\n",
    "    _optim = _params.get('optimizer', torch.optim.SGD(_model.parameters(), lr=_params.get('learning_rate', 0.001)))\n",
    "\n",
    "    # Déplacer le modèle vers le périphérique spécifié (GPU si disponible)\n",
    "    _model.to(device)\n",
    "\n",
    "    # Initialiser des listes pour suivre les pertes et les précisions afin d'analyser les performances\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(_nb_epochs):\n",
    "        _model.train()  # Mettre le modèle en mode entraînement\n",
    "        epoch_training_loss = 0.0\n",
    "\n",
    "        # Étape d'entraînement\n",
    "        for x, t in _training_loader:\n",
    "            x, t = x.to(device), t.to(device)  # Déplacer les données vers le périphérique\n",
    "\n",
    "            # Passer en avant (forward pass)\n",
    "            y = _model(x)\n",
    "\n",
    "            # Calculer la perte\n",
    "            _loss = _loss_func(y, t)\n",
    "            epoch_training_loss += _loss.item()\n",
    "\n",
    "            # Rétropropagation et optimisation\n",
    "            _optim.zero_grad()  # Réinitialiser les gradients\n",
    "            _loss.backward()  # Calculer les gradients\n",
    "            _optim.step()  # Mettre à jour les poids du modèle\n",
    "\n",
    "        # Perte moyenne d'entraînement pour l'époque\n",
    "        avg_training_loss = epoch_training_loss / len(_training_loader)\n",
    "        training_losses.append(avg_training_loss)\n",
    "\n",
    "        # Étape de validation\n",
    "        _model.eval()  # Mettre le modèle en mode évaluation\n",
    "        epoch_val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        # Désactiver le calcul des gradients pendant l'évaluation\n",
    "        with torch.inference_mode():\n",
    "            for data, target in _validation_loader:  # Utiliser validation_loader pour la validation\n",
    "                data, target = data.to(device), target.to(device)  # Déplacer les données vers le périphérique\n",
    "\n",
    "                # Passer en avant (forward pass)\n",
    "                outputs = _model(data)\n",
    "\n",
    "                # Calculer la perte\n",
    "                _loss = _loss_func(outputs, target)\n",
    "                epoch_val_loss += _loss.item()\n",
    "\n",
    "                # Calculer la précision\n",
    "                predicted_labels = torch.argmax(outputs, dim=1)\n",
    "                true_labels = torch.argmax(target, dim=1)\n",
    "                correct_predictions += (predicted_labels == true_labels).sum().item()\n",
    "                total_predictions += target.size(0)\n",
    "\n",
    "        # Perte moyenne de validation et précision pour l'époque\n",
    "        avg_val_loss = epoch_val_loss / len(_validation_loader)\n",
    "        validation_losses.append(avg_val_loss)\n",
    "\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        validation_accuracies.append(accuracy)\n",
    "\n",
    "        # Afficher les métriques pour l'époque en cours\n",
    "        print(f\"Époque {epoch + 1}/{_nb_epochs}, Validation loss: {avg_val_loss:.4f}, Validation accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Calcul de la précision finale sur le jeu de test\n",
    "    final_correct_predictions = 0\n",
    "    final_total_predictions = 0\n",
    "\n",
    "    _model.eval()  # Mettre le modèle en mode évaluation\n",
    "    with torch.inference_mode():\n",
    "        for data, target in _validation_loader:  # Utiliser test_loader pour la phase finale de test\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = _model(data)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            true_labels = torch.argmax(target, dim=1)\n",
    "            final_correct_predictions += (predicted_labels == true_labels).sum().item()\n",
    "            final_total_predictions += target.size(0)\n",
    "\n",
    "    # Calculer la précision finale sur le jeu de test\n",
    "    final_test_accuracy = final_correct_predictions / final_total_predictions\n",
    "    print(f\"Précision finale: {final_test_accuracy:.4f}\")\n",
    "\n",
    "    # Retourner les métriques de performance pour analyse ultérieure\n",
    "    return {\n",
    "        'hyperparameters': _params,\n",
    "        'training_losses': training_losses,\n",
    "        'validation_losses': validation_losses,\n",
    "        'validation_accuracies': validation_accuracies,\n",
    "        'final_validation_accuracy': final_test_accuracy\n",
    "    }\n"
   ],
   "metadata": {
    "id": "qJc122qHBp5g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def setup_deep_network(_params, device='cpu'):\n",
    "    \"\"\"\n",
    "      Prépare les données et intialise l'entrainement du modèle\n",
    "    \"\"\"\n",
    "    _batch_size = _params.get('batch_size', 8)\n",
    "    _nb_neurons_1 = _params.get('nb_neurons_1', 10)\n",
    "    _nb_neurons_2 = _params.get('nb_neurons_2', 10)\n",
    "    _nb_neurons_3 = _params.get('nb_neurons_3', 10)\n",
    "    _nb_epochs =_params.get('nb_epochs', 10)\n",
    "\n",
    "    # Create Loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=_batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=_batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    _model = DeepNeuralNetwork(_nb_neurons_1, _nb_neurons_2, _nb_neurons_3)\n",
    "\n",
    "    # Call training function\n",
    "    print(f\"Training the model : {_params.get('learning_rate', 0.001), _batch_size, _nb_epochs, _nb_neurons_1, _nb_neurons_2, _nb_neurons_3}\")\n",
    "    return train_and_evaluate_2(_model, train_loader, val_loader, _params, device)"
   ],
   "metadata": {
    "id": "hK3CbhWgBwtR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Définir les hyperparamètres et définir une plage de valeurs à explorer"
   ],
   "metadata": {
    "id": "ECtMu-iyCrT3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#configurations des hyperparamètres fixes et des neurones à faire varier\n",
    "param_grid = {\n",
    "    \"nb_neurones_1_range\" :  [10, 20, 32, 64, 128, 256, 512, 1024],\n",
    "    \"nb_neurones_2_range\" :  [10, 20, 32, 64, 128, 256, 512, 1024],\n",
    "    \"nb_neurones_3_range\" :  [10, 20, 32, 64, 128, 256, 512, 1024]\n",
    "}\n",
    "\n",
    "# Combinaisons d'hyperparamètres avec les valeurs fixes pour les autres paramètres\n",
    "learning_rate = 0.01\n",
    "batch_size = 8\n",
    "nb_epochs = 10\n",
    "\n",
    "param_combinations = list(itertools.product(param_grid['nb_neurones_1_range'],\n",
    "                                            param_grid['nb_neurones_2_range'],\n",
    "                                            param_grid['nb_neurones_3_range'])\n",
    "                        )\n"
   ],
   "metadata": {
    "id": "5fgqHfynAOi2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrainement des différents modèles"
   ],
   "metadata": {
    "id": "9In652YMC-4h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Chemin vers le fichier de sortie\n",
    "file_path = \"data/model_metrics_1809024_bis_follow_up.json\"\n",
    "\n",
    "# Vérifie si le fichier existe déjà pour savoir si c'est le premier ajout\n",
    "first_write = not os.path.exists(file_path)\n",
    "\n",
    "res = []\n",
    "\n",
    "# Ouvre le fichier en mode ajout (append) pour écrire progressivement les résultats\n",
    "with open(file_path, \"a\") as json_file:\n",
    "\n",
    "    # Si c'est la première écriture, on ouvre un tableau JSON avec un crochet ouvrant\n",
    "    if first_write:\n",
    "        json_file.write(\"[\\n\")  # Début de l'array JSON\n",
    "\n",
    "    # Boucle sur toutes les combinaisons d'hyperparamètres\n",
    "    for i, (nb_neurons_1, nb_neurons_2, nb_neurons_3) in enumerate(param_combinations):\n",
    "\n",
    "        # Calcule les résultats pour les hyperparamètres actuels\n",
    "        result = setup_deep_network(_params={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"nb_epochs\": nb_epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"nb_neurons_1\": nb_neurons_1,\n",
    "            \"nb_neurons_2\": nb_neurons_2,\n",
    "            \"nb_neurons_3\": nb_neurons_3\n",
    "        })\n",
    "        #\n",
    "        res.append(result)\n",
    "\n",
    "        # Écrit les résultats au format JSON dans le fichier\n",
    "        json.dump(result, json_file)\n",
    "\n",
    "        # Ajoute une virgule et un saut de ligne après chaque entrée, sauf la dernière\n",
    "        if i < len(param_combinations) - 1:\n",
    "            json_file.write(\",\\n\")\n",
    "\n",
    "    # Si c'est la première écriture, on ferme le tableau avec un crochet fermant après la boucle\n",
    "    if first_write:\n",
    "        json_file.write(\"\\n]\")"
   ],
   "metadata": {
    "id": "BiIxov-kCtL6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Amélioration des cinqs meilleurs modèles\n",
    "> On se base sur les observations présenté dans la partie 'Analyse des modèles'."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Re-entrainement des 5 meilleurs modèles\n",
    "best_model_configurations = [\n",
    "    {\"nb_neurons_1\": 1024, \"nb_neurons_2\": 10, \"nb_neurons_3\": 1024},\n",
    "    {\"nb_neurons_1\": 1024, \"nb_neurons_2\": 20, \"nb_neurons_3\": 1024},\n",
    "    {\"nb_neurons_1\": 512, \"nb_neurons_2\": 10, \"nb_neurons_3\": 1024},\n",
    "    {\"nb_neurons_1\": 256, \"nb_neurons_2\": 10, \"nb_neurons_3\": 1024},\n",
    "    {\"nb_neurons_1\": 1024, \"nb_neurons_2\": 10, \"nb_neurons_3\": 512}\n",
    "]\n",
    "\n",
    "#hyperparamètres fixes\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "nb_epochs = 50\n",
    "\n",
    "# Chemin vers le fichier de sortie\n",
    "file_path = \"data/model_deep_reentrainement_top5.json\"\n",
    "\n",
    "# Vérifie si le fichier existe déjà pour savoir si c'est le premier ajout\n",
    "first_write = not os.path.exists(file_path)\n",
    "\n",
    "res = []\n",
    "\n",
    "# Ouvre le fichier en mode ajout (append) pour écrire progressivement les résultats\n",
    "with open(file_path, \"a\") as json_file:\n",
    "\n",
    "    # Si c'est la première écriture, on ouvre un tableau JSON avec un crochet ouvrant\n",
    "    if first_write:\n",
    "        json_file.write(\"[\\n\")  # Début de l'array JSON\n",
    "        \n",
    "    # Boucle sur les 5 meilleurs modèles\n",
    "    for config in best_model_configurations:\n",
    "        nb_neurons_1 = config[\"nb_neurons_1\"]\n",
    "        nb_neurons_2 = config[\"nb_neurons_2\"]\n",
    "        nb_neurons_3 = config[\"nb_neurons_3\"]\n",
    "        \n",
    "        print(f\"Réentraînement du modèle avec nb_neurons_1 :{nb_neurons_1} , nb_neurons_2 : {nb_neurons_2} et nb_neurons_3: {nb_neurons_3}\")\n",
    "        \n",
    "        # Calcul des résultats pour les hyperparamètres actuels\n",
    "        result = setup_deep_network(_params={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"nb_epochs\": nb_epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"nb_neurons_1\": nb_neurons_1,\n",
    "            \"nb_neurons_2\": nb_neurons_2,\n",
    "            \"nb_neurons_3\": nb_neurons_3\n",
    "        })\n",
    "        \n",
    "        res.append(result)\n",
    "\n",
    "        # Écrit les résultats au format JSON dans le fichier\n",
    "        json.dump(result, json_file)\n",
    "\n",
    "        # Ajoute une virgule et un saut de ligne après chaque entrée, sauf la dernière\n",
    "        json_file.write(\",\\n\")\n",
    "        \n",
    "\n",
    "    # Si c'est la première écriture, on ferme le tableau avec un crochet fermant après la boucle\n",
    "    if first_write:\n",
    "        json_file.write(\"\\n]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyse des modèles"
   ],
   "metadata": {
    "id": "AggIe2-EJOXF"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Deux couches cachées"
  },
  {
   "cell_type": "code",
   "source": [
    "#on transforme les données du fichier en dataframe\n",
    "with open('data/model_metrics_DL_2L.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "#on crée une liste vide pour stocker les données structurées\n",
    "structured_data = []\n",
    "\n",
    "# on parcourt les données du fichier\n",
    "for entry in data:\n",
    "    # on extrait les hypermatères\n",
    "    hyperparameters = entry['hyperparameters']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    nb_epochs = hyperparameters['nb_epochs']\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    nb_neurons_1 = hyperparameters['nb_neurons_1']\n",
    "    nb_neurons_2 = hyperparameters['nb_neurons_2']\n",
    "    # nb_neurons_3 = hyperparameters['nb_neurons_3']\n",
    "    training_losses = entry['training_losses']\n",
    "    validation_losses = entry['validation_losses']\n",
    "    validation_accuracies = entry['validation_accuracies']\n",
    "    final_validation_accuracy = entry['final_validation_accuracy']\n",
    "\n",
    "    # on crée un dictionnaire avec les données structurées\n",
    "    structured_data.append({\n",
    "        'batch_size': batch_size,\n",
    "        'nb_epochs': nb_epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'nb_neurons_1': nb_neurons_1,\n",
    "        'nb_neurons_2': nb_neurons_2,\n",
    "        # 'nb_neurons_3': nb_neurons_3,\n",
    "        'training_losses': training_losses,\n",
    "        'validation_losses': validation_losses,\n",
    "        'validation_accuracies': validation_accuracies,\n",
    "        'final_validation_accuracy': final_validation_accuracy\n",
    "    })\n",
    "\n",
    "# on crée un dataframe avec les données structurées\n",
    "df = pd.DataFrame(structured_data)\n",
    "df.sort_values(\"final_validation_accuracy\", ascending=False, inplace=True)"
   ],
   "metadata": {
    "id": "kQqp7_ZSJQhD",
    "ExecuteTime": {
     "end_time": "2024-10-16T09:43:01.943631Z",
     "start_time": "2024-10-16T09:43:01.880238Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:05:59.298701Z",
     "start_time": "2024-10-16T10:05:59.266290Z"
    }
   },
   "cell_type": "code",
   "source": "df[['learning_rate', 'batch_size', 'nb_epochs', 'nb_neurons_1', 'nb_neurons_2', 'final_validation_accuracy']]\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    learning_rate  batch_size  nb_epochs  nb_neurons_1  nb_neurons_2  \\\n",
       "76           0.01          32         25          2048           128   \n",
       "80           0.01          32         25          2048          2048   \n",
       "79           0.01          32         25          2048          1024   \n",
       "70           0.01          32         25          1024          1024   \n",
       "64           0.01          32         25          1024            20   \n",
       "..            ...         ...        ...           ...           ...   \n",
       "4            0.01          32         25            10           128   \n",
       "3            0.01          32         25            10            64   \n",
       "2            0.01          32         25            10            32   \n",
       "1            0.01          32         25            10            20   \n",
       "0            0.01          32         25            10            10   \n",
       "\n",
       "    final_validation_accuracy  \n",
       "76                   0.975159  \n",
       "80                   0.974762  \n",
       "79                   0.974524  \n",
       "70                   0.974444  \n",
       "64                   0.974444  \n",
       "..                        ...  \n",
       "4                    0.939206  \n",
       "3                    0.936746  \n",
       "2                    0.934683  \n",
       "1                    0.931587  \n",
       "0                    0.923889  \n",
       "\n",
       "[81 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>nb_epochs</th>\n",
       "      <th>nb_neurons_1</th>\n",
       "      <th>nb_neurons_2</th>\n",
       "      <th>final_validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>2048</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.974762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.974524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.974444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>0.974444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.939206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.936746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.934683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.931587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.923889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trois couches cachées"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T09:49:27.310265Z",
     "start_time": "2024-10-16T09:49:27.277567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#on transforme les données du fichier en dataframe\n",
    "with open('data/model_metrics_DL_Top_5.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "# with open('data/model_metrics_DL_3L.json') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "\n",
    "#on crée une liste vide pour stocker les données structurées\n",
    "structured_data = []\n",
    "\n",
    "# on parcourt les données du fichier\n",
    "for entry in data:\n",
    "    # on extrait les hypermatères\n",
    "    hyperparameters = entry['hyperparameters']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    nb_epochs = hyperparameters['nb_epochs']\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    nb_neurons_1 = hyperparameters['nb_neurons_1']\n",
    "    nb_neurons_2 = hyperparameters['nb_neurons_2']\n",
    "    nb_neurons_3 = hyperparameters['nb_neurons_3']\n",
    "    training_losses = entry['training_losses']\n",
    "    validation_losses = entry['validation_losses']\n",
    "    validation_accuracies = entry['validation_accuracies']\n",
    "    final_validation_accuracy = entry['final_validation_accuracy']\n",
    "\n",
    "    # on crée un dictionnaire avec les données structurées\n",
    "    structured_data.append({\n",
    "        'batch_size': batch_size,\n",
    "        'nb_epochs': nb_epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'nb_neurons_1': nb_neurons_1,\n",
    "        'nb_neurons_2': nb_neurons_2,\n",
    "        'nb_neurons_3': nb_neurons_3,\n",
    "        'training_losses': training_losses,\n",
    "        'validation_losses': validation_losses,\n",
    "        'validation_accuracies': validation_accuracies,\n",
    "        'final_validation_accuracy': final_validation_accuracy\n",
    "    })\n",
    "\n",
    "# on crée un dataframe avec les données structurées\n",
    "df_deep = pd.DataFrame(structured_data)\n",
    "df_deep.sort_values(\"final_validation_accuracy\", ascending=False, inplace=True)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "df_deep.head(5)[['learning_rate', 'batch_size', 'nb_epochs', 'nb_neurons_1', 'nb_neurons_2', 'final_validation_accuracy']]",
   "metadata": {
    "id": "tz9TvMcIJd74",
    "ExecuteTime": {
     "end_time": "2024-10-16T09:54:00.795813Z",
     "start_time": "2024-10-16T09:54:00.779909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   learning_rate  batch_size  nb_epochs  nb_neurons_1  nb_neurons_2  \\\n",
       "0           0.01          32         50          1024            10   \n",
       "1           0.01          32         50          1024            20   \n",
       "4           0.01          32         50          1024            10   \n",
       "2           0.01          32         50           512            10   \n",
       "3           0.01          32         50           256            10   \n",
       "\n",
       "   final_validation_accuracy  \n",
       "0                   0.979921  \n",
       "1                   0.979841  \n",
       "4                   0.978810  \n",
       "2                   0.978333  \n",
       "3                   0.978333  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>nb_epochs</th>\n",
       "      <th>nb_neurons_1</th>\n",
       "      <th>nb_neurons_2</th>\n",
       "      <th>final_validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>0.979921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>0.979841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test final"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T09:58:21.003277Z",
     "start_time": "2024-10-16T09:54:56.669669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Params\n",
    "_nb_neurons_1 = 1024\n",
    "_nb_neurons_2 = 10\n",
    "_nb_neurons_3 = 1024\n",
    "_learning_rate = 0.01\n",
    "_batch_size = 32\n",
    "_nb_epochs = 50\n",
    "\n",
    "# Load data\n",
    "((data_train,label_train),(data_test,label_test)) = torch.load(gzip.open('data/mnist.pkl.gz'), weights_only=False)\n",
    "training_dataset = torch.utils.data.TensorDataset(data_train, label_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(data_test, label_test)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "generator = torch.Generator().manual_seed(42) # Permet de reproduire le même découpement\n",
    "train_subset, val_subset = torch.utils.data.random_split(training_dataset, [0.8, 0.2], generator=generator)\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=_batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=_batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model = DeepNeuralNetwork(_nb_neurons_1, _nb_neurons_2, _nb_neurons_3)\n",
    "\n",
    "#\n",
    "_loss_func = torch.nn.CrossEntropyLoss()\n",
    "_optimizer = torch.optim.SGD(model.parameters(), lr=_learning_rate)\n",
    "\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(_nb_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for x, t in train_loader:\n",
    "        x, t = x.to(device), t.to(device)  # Move data to device\n",
    "\n",
    "        # Forward pass\n",
    "        y = model(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = _loss_func(y, t)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        _optimizer.zero_grad()  # Reset gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        _optimizer.step()  # Update model weights\n",
    "\n",
    "    # Validation step (optional)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.inference_mode():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = _loss_func(outputs, target)\n",
    "\n",
    "# save model\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path\n",
    "MODEL_NAME = \"deep_neural_network.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)\n",
    "\n",
    "acc = 0.\n",
    "# on lit toutes les donnéees de test\n",
    "for x,t in test_loader:\n",
    "  # on calcule la sortie du modèle\n",
    "  y = model(x)\n",
    "  # on regarde si la sortie est correcte\n",
    "  acc += torch.argmax(y,1) == torch.argmax(t,1)\n",
    "# on affiche le pourcentage de bonnes réponses\n",
    "print(acc/data_test.shape[0])"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 45\u001B[0m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;66;03m# Backpropagation and optimization\u001B[39;00m\n\u001B[0;32m     44\u001B[0m     _optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Reset gradients\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Compute gradients\u001B[39;00m\n\u001B[0;32m     46\u001B[0m     _optimizer\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# Update model weights\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Validation step (optional)\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\MLSandbox\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\MLSandbox\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importer le modèle"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:26:04.477271Z",
     "start_time": "2024-10-18T18:26:04.387849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Params\n",
    "_nb_neurons_1 = 1024\n",
    "_nb_neurons_2 = 10\n",
    "_nb_neurons_3 = 1024\n",
    "\n",
    "# Load the model\n",
    "model = DeepNeuralNetwork(_nb_neurons_1, _nb_neurons_2, _nb_neurons_3)\n",
    "model.load_state_dict(torch.load(\"models/model_deep_network.pth\"))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:27:50.232979Z",
     "start_time": "2024-10-18T18:27:49.045834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "((data_train,label_train),(data_test,label_test)) = torch.load(gzip.open('data/mnist.pkl.gz'), weights_only=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(data_test, label_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:27:54.063964Z",
     "start_time": "2024-10-18T18:27:50.254906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acc = 0.\n",
    "# on lit toutes les donnéees de test\n",
    "for x,t in test_loader:\n",
    "  # on calcule la sortie du modèle\n",
    "  y = model(x)\n",
    "  # on regarde si la sortie est correcte\n",
    "  acc += torch.argmax(y,1) == torch.argmax(t,1)\n",
    "# on affiche le pourcentage de bonnes réponses\n",
    "print(acc/data_test.shape[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9806])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
